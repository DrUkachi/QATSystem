{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the Test Notebook to test the API\n",
    "\n",
    "Before you use the notebook please ensure you have\n",
    "\n",
    "1. Followed the instructions on the README\n",
    "2. The Postgres Database is up and running - if you restarted your laptop, device or environment you can use this\n",
    "```bash\n",
    "sudo service postgresql start\n",
    "```\n",
    "If you are using Windows, just start the Postgres Software\n",
    "\n",
    "To confirm that it works you can use this\n",
    "\n",
    "```bash\n",
    "psql -U your_username -d your_database_name\n",
    "```\n",
    "You the put your password and ensure its at least connected\n",
    "\n",
    "\n",
    "3. The flask app has been started. You can use the following command to get started\n",
    "\n",
    "```bash\n",
    "python flask_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Set the base URL for the Flask API\n",
    "BASE_URL = \"http://localhost:8080\"  # Adjust as necessary\n",
    "\n",
    "def test_upload_file(file_path):\n",
    "    \"\"\"Test the file upload endpoint.\"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        response = requests.post(f\"{BASE_URL}/upload\", files={\"file\": file})\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Upload successful:\", response.json())\n",
    "    else:\n",
    "        print(\"Upload failed:\", response.status_code, response.json())\n",
    "    \n",
    "    return response\n",
    "\n",
    "def test_process_query(query):\n",
    "    \"\"\"Test the process query endpoint.\"\"\"\n",
    "    payload = {\"query\": query}\n",
    "    response = requests.post(f\"{BASE_URL}/query\", json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Query processing successful:\", response.json())\n",
    "    else:\n",
    "        print(\"Query processing failed:\", response.status_code, response.json())\n",
    "    \n",
    "    return response\n",
    "\n",
    "def test_evaluate_user_answer(test_question_id, user_answer):\n",
    "    \"\"\"Test the evaluate user answer endpoint.\"\"\"\n",
    "    payload = {\n",
    "        \"answer\": user_answer,\n",
    "        \"test_question_id\": test_question_id\n",
    "    }\n",
    "    response = requests.post(f\"{BASE_URL}/evaluate\", json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Evaluation successful:\", response.json())\n",
    "    else:\n",
    "        print(\"Evaluation failed:\", response.status_code, response.json())\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_path = \"/teamspace/studios/this_studio/QATSystem/sample_file/2005.11401v4.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_upload_file(sample_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query processing successful: {'answer': \"Based on the documents provided, there are a few potential disadvantages of RAG (Retrieval-Augmented Generation) that can be identified:\\n\\n1. Inefficient decoding for longer output sequences: For RAG-Sequence, the likelihood of an output sequence doesn't break down into a per-token likelihood, which makes it difficult to solve with a single beam search. Instead, beam search is run for each document, which can be computationally expensive, especially for longer output sequences.\\n2. Dependence on the quality of the retrieved documents: RAG's performance is directly tied to the quality of the documents it retrieves. If the retrieved documents are not highly relevant or informative, RAG's output will likely be less accurate or informative as well.\\n3. Potential for misuse: As with any language model, RAG could potentially be used to generate abuse, fake, or misleading content. This is a potential downside to any technology that can generate human-like text.\\n4. Limitations of the knowledge source: While RAG is grounded in real factual knowledge from a source like Wikipedia, this knowledge source is not entirely factual and completely devoid of bias. This means that RAG's output may still contain errors or biases that are present in the original knowledge source.\\n\\nIt's worth noting that some of these disadvantages may be mitigated by further research and development. For example, more efficient decoding algorithms could be developed to reduce the computational cost of RAG, or methods could be developed to improve the quality of the retrieved documents. However, these are some of the potential disadvantages based on the current state of the technology.\", 'bullet_points': \"Sure, here are three bullet points to further elaborate on the potential disadvantages of RAG:\\n\\n* Bullet Point 1: Inefficient decoding for longer output sequences: RAG's decoding process involves running beam search for each retrieved document, which can be computationally expensive for longer output sequences. This is because the likelihood of an output sequence doesn't break down into a per-token likelihood, making it difficult to solve with a single beam search. As a result, the number of forward passes required can become very large, leading to decreased efficiency and increased computational cost.\\n* Bullet Point 2: Dependence on the quality of the retrieved documents: RAG's performance is directly tied to the quality of the documents it retrieves. If the retrieved documents are not highly relevant or informative, RAG's output will likely be less accurate or informative as well. This means that if the retrieval system fails to find high-quality documents, RAG's output may be subpar. Additionally, if the retrieved documents contain errors or biases, RAG's output may also contain these errors or biases.\\n* Bullet Point 3: Limitations of the knowledge source: While RAG is grounded in real factual knowledge from a source like Wikipedia, this knowledge source is not entirely factual and completely devoid of bias. This means that RAG's output may still contain errors or biases that are present in the original knowledge source. Furthermore, RAG is only as good as the knowledge source it is trained on. If the knowledge source is limited or incomplete, RAG's output may also be limited or incomplete. Additionally, RAG's output may be limited by the scope of the knowledge source, meaning that it may not be able to answer questions or generate text about topics that are not covered in the knowledge source.\", 'test_question': 'How does the dependence on the quality of retrieved documents impact the performance of RAG, and are there any potential solutions to this issue?', 'test_question_id': '4f13f0cb-b925-442e-8c7c-43e90fe4ed1b'}\n"
     ]
    }
   ],
   "source": [
    "response = test_process_query(\"What are the disadvantages of RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"Based on the documents provided, there are a few potential disadvantages of RAG (Retrieval-Augmented Generation) that can be identified:\\n\\n1. Inefficient decoding for longer output sequences: For RAG-Sequence, the likelihood of an output sequence doesn't break down into a per-token likelihood, which makes it difficult to solve with a single beam search. Instead, beam search is run for each document, which can be computationally expensive, especially for longer output sequences.\\n2. Dependence on the quality of the retrieved documents: RAG's performance is directly tied to the quality of the documents it retrieves. If the retrieved documents are not highly relevant or informative, RAG's output will likely be less accurate or informative as well.\\n3. Potential for misuse: As with any language model, RAG could potentially be used to generate abuse, fake, or misleading content. This is a potential downside to any technology that can generate human-like text.\\n4. Limitations of the knowledge source: While RAG is grounded in real factual knowledge from a source like Wikipedia, this knowledge source is not entirely factual and completely devoid of bias. This means that RAG's output may still contain errors or biases that are present in the original knowledge source.\\n\\nIt's worth noting that some of these disadvantages may be mitigated by further research and development. For example, more efficient decoding algorithms could be developed to reduce the computational cost of RAG, or methods could be developed to improve the quality of the retrieved documents. However, these are some of the potential disadvantages based on the current state of the technology.\",\n",
       " 'bullet_points': \"Sure, here are three bullet points to further elaborate on the potential disadvantages of RAG:\\n\\n* Bullet Point 1: Inefficient decoding for longer output sequences: RAG's decoding process involves running beam search for each retrieved document, which can be computationally expensive for longer output sequences. This is because the likelihood of an output sequence doesn't break down into a per-token likelihood, making it difficult to solve with a single beam search. As a result, the number of forward passes required can become very large, leading to decreased efficiency and increased computational cost.\\n* Bullet Point 2: Dependence on the quality of the retrieved documents: RAG's performance is directly tied to the quality of the documents it retrieves. If the retrieved documents are not highly relevant or informative, RAG's output will likely be less accurate or informative as well. This means that if the retrieval system fails to find high-quality documents, RAG's output may be subpar. Additionally, if the retrieved documents contain errors or biases, RAG's output may also contain these errors or biases.\\n* Bullet Point 3: Limitations of the knowledge source: While RAG is grounded in real factual knowledge from a source like Wikipedia, this knowledge source is not entirely factual and completely devoid of bias. This means that RAG's output may still contain errors or biases that are present in the original knowledge source. Furthermore, RAG is only as good as the knowledge source it is trained on. If the knowledge source is limited or incomplete, RAG's output may also be limited or incomplete. Additionally, RAG's output may be limited by the scope of the knowledge source, meaning that it may not be able to answer questions or generate text about topics that are not covered in the knowledge source.\",\n",
       " 'test_question': 'How does the dependence on the quality of retrieved documents impact the performance of RAG, and are there any potential solutions to this issue?',\n",
       " 'test_question_id': '4f13f0cb-b925-442e-8c7c-43e90fe4ed1b'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question_id = response.json()['test_question_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see that the evaluation works you can use two examples\n",
    "\n",
    "A wrong answer and a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_generated_answer = \"\"\"I don't know the answer\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_generated_answer = \"\"\"The quality of retrieved documents significantly impacts the performance of RAG (Retrieval Augmented Generation) models. If the retrieved documents are irrelevant, inaccurate, or incomplete, the model will likely generate incorrect or misleading responses. Therefore, it's crucial to ensure the quality of the document corpus used for RAG.\n",
    "\n",
    "Here are some potential solutions to mitigate the dependence on document quality:\n",
    "\n",
    "Document Filtering and Curation: Implement mechanisms to filter and curate the document corpus, removing irrelevant or low-quality documents. This can involve using keyword matching, topic modeling, or other techniques to identify relevant documents.\n",
    "\n",
    "Document Ranking: Rank retrieved documents based on their relevance to the query. This can be achieved using techniques like cosine similarity, TF-IDF, or more advanced ranking algorithms. By prioritizing relevant documents, the model can generate more accurate responses.\n",
    "\n",
    "Multiple Document Retrieval: Retrieve multiple documents related to the query and combine their information to generate a more comprehensive response. This can help mitigate the impact of individual document quality issues.\n",
    "\n",
    "Contextual Awareness: Enhance the model's ability to understand the context of the query and the retrieved documents. This can involve using techniques like semantic analysis or knowledge graphs to identify relationships between concepts and information.\n",
    "\n",
    "Feedback Mechanisms: Incorporate feedback mechanisms to allow users to provide feedback on the generated responses. This feedback can be used to improve the model's performance over time by identifying and addressing issues related to document quality and response accuracy.\n",
    "\n",
    "By addressing these factors, RAG models can become more robust and less reliant on the quality of individual documents, leading to improved performance and more accurate responses..\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation successful: {'knowledge_confidence': 81, 'knowledge_understood': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluate_user_answer(test_question_id,\n",
    "correct_generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation successful: {'knowledge_confidence': 11, 'knowledge_understood': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_evaluate_user_answer(test_question_id,\n",
    "wrong_generated_answer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
